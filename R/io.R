#' Write a copernicus file given a stars object, database and path
#' 
#' @export
#' @param x a stars object
#' @param db database table
#' @param path chr, the output data path used by [compose_filename]
#' @param check_path logical, if TRUE then make sure the output path exists.  Note 
#'   the final output path is generated by [compose_filename] and may not be the same
#'   as `path`.
#' @return the input stars object
write_copernicus = function(x, db, path = ".", check_path = TRUE){
  if (inherits(db, "merged") || "product" %in% colnames(db)) {
    stop("writing from a merged database not permitted")
  }
  fname = compose_filename(db, path)
  if (check_path) ok = make_path(dirname(fname))
  stars::write_stars(x, fname)
}

#' @export
#' @rdname write_copernicus
write_andreas = function(x, db, path = ".", check_path = TRUE){
  write_copernicus(x, db, path = ".", check_path = TRUE)
}


#' Read one or more copernicus files
#' 
#' By default the function tries to return an object with (variables of x, y, time) 
#' dimensions.  If multiple times are provided, then each var must be equally
#' represented.
#' 
#' @export
#' @param db tibble, database of selected records
#' @param path char, the path to the data set
#' @param crs value to set the CRS
#' @param bb NULL or and object used for cropping
#' @param ... other arguments for `stars::st_crop`
#' @return stars object
read_copernicus = function(db, path, crs = 4326, bb = NULL){
  db$datetime = as.POSIXct(paste(format(db$date, '%Y-%m-%d'), db$time), 
                       format = "%Y-%m-%d %H%M%S", tz = 'UTC')  
  db$file = compose_filename(db, path)
  # read each variable
  # check that each variable has the same time-dim
  # if ok then bind, otherwise error
  r = db |>
    dplyr::group_by(.data$variable) |>
    dplyr::group_map(
      function(tbl, key){
        if (nrow(tbl) > 1 ){
          s = stars::read_stars(tbl$file, along = list(time = tbl$datetime)) |>
            rlang::set_names(tbl$variable[1])
        } else {
          s = stars::read_stars(tbl$file) |>
            rlang::set_names(tbl$variable[1])
        }
      }, .keep = TRUE) |>
    copernicus::bind_stars() |>
    sf::st_set_crs(crs)
  
  if (!is.null(bb)) {
    orig = sf::sf_use_s2(FALSE)  # in case the r bb extends beyond a pole
    r = stars::st_crop(r, bb, ...)
    dummy = sf::sf_use_s2(orig)
  }
  r
}

#' @export
#' @rdname read_copernicus
read_andreas = function(db, path, ...){
  read_copernicus(db, path, ...)
}


#' Unpack a copernicus ncdf4 file
#'
#' @export
#' @param filename character, the full path specification
#' @param banded logical, see \code{\link{get_var}}.  Setting to FALSE
#'        returns single band objects (depth and time dropped)
#' @param bind logical, if TRUE bind the outputs into one stars object
#' @return list of \code{stars} objects
unpack_copernicus <- function(filename, banded = FALSE, bind = TRUE){
  #x <- ncdf4::nc_open(filename[1])
  #ss <- sapply(get_varnames(x),
  #             function(vname){
  #               get_var(x, var = vname, banded = banded)
  #             },
  #             simplify = FALSE)
  #ncdf4::nc_close(x)
  #if (bind) ss = bind_stars(ss)
  stars::read_stars(filename)
}

#' Archive a stars object to a database
#' 
#' @export
#' @param x stars object
#' @param lut a tabular database (ala from `read_product_lut()`)
#' @param path char, the data path
#' @param time NULL
#' @param ... arguments for \code{\link{generate_filename}}
#' @return tabular database as a tibble
archive_andreas = function(x, lut,
                           path = ".",
                           time = NULL,
                           ...){
  
  dates = stars::st_get_dimension_values(x, "time") 
  if (is.null(dates)) {
    if (is.null(time)){
      stop("Either `x` should have a `time` dimension or you should provide time")
    }
    dates = time
  }
  
  vnames = names(x)
  lnames = unique(lut$name)
  ix = vnames %in% lnames
  if (!all(ix)){
    stop("all variable names in `x` must be found in `lut`: ", 
         paste(vnames[!ix], collapse = ", "))
  }
  
  
  db = lapply(vnames,
              function(vname){
                minilut = dplyr::filter(lut,
                                        name == vname)
                ff = file.path(path,
                               format(x$date, "%Y/%m%d"),
                               sprintf("%s__%s_%s_%s_%s_%s%s",
                                       x$id,
                                       sprintf("%sT%s", format(x$date, "%Y-%m-%d"), x$time),
                                       x$depth, 
                                       x$period,
                                       x$variable,
                                       x$treatment,
                                       ext))
              })
  
  
  # <path>/YYYY/mmdd/id__datetime_depth_period_variable_treatment.ext
  
  ff = file.path(path,
            format(x$date, "%Y/%m%d"),
            sprintf("%s__%s_%s_%s_%s_%s%s",
                    x$id,
                    sprintf("%sT%s", format(x$date, "%Y-%m-%d"), x$time),
                    x$depth, 
                    x$period,
                    x$variable,
                    x$treatment,
                    ext))
  
  ff = generate_filename(x, ...)
  db = decompose_filename(ff)
  ff = compose_filename(db, path)
  # for each time, variable, depth order in filename
  d = dim(x)
  i = 1
  n = length(x)
  for (idepth in seq_len(d[['depth']])){
    for (itime in seq_len(d[['time']])){
      for (v in (seq_len(n))){
        if (!dir.exists(dirname(ff[i+v]))) dir.create(dirname(ff[i+v]),
                                                      recursive = TRUE)
        stars::write_stars(x[v, , ,idepth, itime, drop = TRUE], ff[i+v-1])
      }
      i = i + n # advance the count
    } # iyear
  } # idepth
  
  db
  
}