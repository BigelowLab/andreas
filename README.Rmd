---
title: "andreas"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Andreas (Andrew) Copernicus was the lesser known brother of [Nicolaus Copernicus](https://en.wikipedia.org/wiki/Nicolaus_Copernicus).  This package serves as suite of functions and scripts to access and manage data from the [Copernicus Marine Data Store](https://data.marine.copernicus.eu/products).  This package leverages the tools provided by the [`copernicus` R package](https://github.com/BigelowLab/copernicus).


## Look up tables

We use look up tables (`lut`), merged with Copernicus product description tables, to allow users to define a subset of data sets.  These `luts` can be configured as needed, but for out use case we want to know the `dataset_id`, `variable`, the `depth` and `time` to request.  The spatial bounding box coordinates are not maintained in our default tables, but you could manage those elsewhere (we do using the [cofbb R package](https://github.com/BigelowLab/cofbb)).  Below is out example we use for fetching data from the [GLOBAL_ANALYSISFORECAST_PHY_001_024](https://data.marine.copernicus.eu/product/GLOBAL_ANALYSISFORECAST_PHY_001_024/description) product suite.

```{r}
suppressPackageStartupMessages({
  library(andreas)
  library(copernicus)
  library(dplyr)
  library(stars)
})

lut = read_product_lut(product_id = 'GLOBAL_ANALYSISFORECAST_PHY_001_024') |>
  glimpse()
```

Some of the variables are manually curated: `depth`, `fetch`, `mindepth` and `maxdepth`. The balance are derived from the coperncius catalog where they are described: `dataset_id`, `dataset_name`, `short_name`, `standard_name`, `units`, `start_time` and `end_time`.  The latter two tell us the dates between which the products are known to be served.

The manually curated ones are ..

`depth` a descriptive variable with values including 'sur', 'bot' and 'mld', but you could store a numeric depth, too.  The only thing to keep in mind is the numeric depth would be cast to character class.  

`fetch` is simple "yes" or "no" - this way you can turn them on and off as needed to fetch data.

`mindepth` and `maxdepth` are passed to the copernicus package as min/max filters for searching.

The `name` field is a automatically curated field, we convert `short_name` to camelCase to purge the underscores (to save annoyance later when archiving files.)

To be super clear, somebody has to maintain a look up table - it requires some manual intervention. And you can designed the `LUT` to serve your own needs. But once the table is set up, it can persist between R sessions, as we save them in the `copernicus::copernicus_path("lut")` directory.


## Fetch data

Requests for data from the [Copernicus Marine Data Store](https://data.marine.copernicus.eu/products) are as simple as filtering the above LUT for the desired records (rows).  We use `fetch_andreas()` for this, but keep in mind that it is not always possible to bind attrributes (variables in a stars object) if variables are of varying dimensionality - in particular if they have a depth dimension when retrieved in native form.  

```{r}
x = lut |>
  dplyr::filter(name %in% c("uo", "vo", "thetao", "mlotst")) |>
  fetch_andreas(time = Sys.Date() + c(0,1), 
                bb = cofbb::get_bb("gom"))
x
```

## Archiving data

You can download and archive data using the database functionality provided in this package.  There are a number of ways to manage suites of data, this is just one fairly light weight method.

Here, we store data in a directory tree that starts with `region` and `product` at it's root. Within the `product` we divide by `year`, `monthday`.  Within in each `monthday` directory there are one or more files uniquely named to provide complete identification of datasetid, time, depth, period, variable and treatment. Each file contains one raster for one variable at one depth and one time.   

Here is an example of a file name that follows this pattern `datasetid__date_time_depth_period_variable_treatment.tif`.
```
nwa/GLOBAL_ANALYSISFORECAST_PHY_001_024/2022/0601/cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m__2022-06-01T000000_sur_day_uo_raw.tif
```

Here you can see that data set id and the rest of the identifiers are separated by a double underscore to aid in programmatic parsing.  Time includes the hour in case we ever want to download the 6-hour data sets.  Depth is currently only expressed as "sur", "bot" and "mld", but use of measured values such as "1.493" *etc.* is allowed but not currently used. The treatment, `raw`, in this case means the values are as downloaded, however, if you ever wanted to roll your own running mean (say 8-day rolling mean) or some other statistic this naming system provides the flexibility you will need.

**NOTE** Don't forget to [set your root data path](https://github.com/BigelowLab/copernicus?tab=readme-ov-file#configure-data-path.).

First we define an output path for the Gulf of Maine data.  The path isn't created until data is written to it.  Then we simply call fetch and write individual GeoTIFF files into a database structure.  Note that we provide an identifier that provides the provenance of the data.  We receive, in turn, a table that serves as a database.



```{r archive}
x = stars::read_stars(ofile) |>
  dplyr::slice("depth", 1)
dates = stars::st_get_dimension_values(x, "time")
db = lapply(names(x),
  function(nm){
    db = sprintf("%s__%s_%s_%s_%s_%s%s",
                 dataset_id,
                 format(dates, "%Y-%m-%dT000000"),
                 "sur", 
                 "day",
                 nm,
                 "raw",
                 ".tif") |>
      decompose_filename()
    for(i in seq_along(dates)) write_copernicus(dplyr::slice(x[nm], "time", i), 
                                                dplyr::slice(db, i),
                                                path)
    db
  }) |>
  dplyr::bind_rows()
```

Since this is the first time you have downloaded and archived data, be sure to save the database.

```{r write_database}
write_database(db, path)
```

### Using the database

The database is very light and easy to filter for just the records you might need. Note that depth is a character data type; this provides you with flexibility to define depth as 'surface' or '50-75' or something like that.

Let's walk through reading the database, filtering it for a subset, reading the files and finally displaying.

```{r database, message = FALSE}
db <- copernicus::read_database(path) |>
  dplyr::glimpse()
```

Now we can read in the files.

```{r read_files}
s = read_copernicus(db, path)
s
```

The database diretcory structure looks like this (requires [fs package](https://CRAN.R-project.org/package=fs)) which you won't need just to use copernicus).

```{r dir_tree}
fs::dir_tree(path)
```

It may look complicated, but this permits the storage of multiple datasets per product, and is a very flexible system.