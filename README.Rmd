---
title: "andreas"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Archiving data

You can download and archive data using the database functionality provided in this package.  There are a number of ways to manage suites of data, this is just one fairly light weight method.

Here, we store data in a directory tree that starts with `product` and `region` at it's root. Within the `region` we divide by `year`, `monthday`.  Within in each `monthday` directory there are one or more files uniquely named to provide complete identification of datasetid, time, depth, period, variable and treatment. Each file contains one raster for one variable at one depth and one time.   

Here is an example of a file name that follows this pattern `datasetid__date_time_depth_period_variable_treatment.tif`.
```
GLOBAL_ANALYSISFORECAST_PHY_001_024/nwa/2022/0601/cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m__2022-06-01T000000_sur_day_uo_raw.tif
```

Here you can see that data set id and the rest of the identifiers are separated by a double underscore to aid in programmatic parsing.  Time includes the hour in case we ever want to download the 6-hour data sets.  Depth is currenly only expressed as "sur", "bot" and "mld", but use of measured values such as "1.493" *etc.* is allowed but not currently used. The treatment, `raw`, in this case means the values are as downloaded, however, if you ever wanted to roll your own running mean (say 8-day rolling mean) or some other statistic this naming system provides the flexibility you will need.

**NOTE** Don't forget to [set your root data path](#Configure-data-path).

First we define an output path for the Gulf of Maine data.  The path isn't created until data is written to it.  Then we simply call fetch and write individual GeoTIFF files into a database structure.  Note that we provide an identifier that provides the provenance of the data.  We receive, in turn, a table that serves as a database.



```{r archive}
x = stars::read_stars(ofile) |>
  dplyr::slice("depth", 1)
dates = stars::st_get_dimension_values(x, "time")
db = lapply(names(x),
  function(nm){
    db = sprintf("%s__%s_%s_%s_%s_%s%s",
                 dataset_id,
                 format(dates, "%Y-%m-%dT000000"),
                 "sur", 
                 "day",
                 nm,
                 "raw",
                 ".tif") |>
      decompose_filename()
    for(i in seq_along(dates)) write_copernicus(dplyr::slice(x[nm], "time", i), 
                                                dplyr::slice(db, i),
                                                path)
    db
  }) |>
  dplyr::bind_rows()
```

Since this is the first time you have downloaded and archived data, be sure to save the database.

```{r write_database}
write_database(db, path)
```

### Using the database

The database is very light and easy to filter for just the records you might need. Note that depth is a character data type; this provides you with flexibility to define depth as 'surface' or '50-75' or something like that.

Let's walk through reading the database, filtering it for a subset, reading the files and finally displaying.

```{r database, message = FALSE}
db <- copernicus::read_database(path) |>
  dplyr::glimpse()
```

Now we can read in the files.

```{r, read_files}
s = read_copernicus(db, path)
s
```

The database diretcory structure looks like this (requires [fs package](https://CRAN.R-project.org/package=fs)) which you won't need just to use copernicus).

```{r dir_tree}
fs::dir_tree(path)
```

It may look complicated, but this permits the storage of multiple datasets per product, and is a very flexible system.